{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install instructor pydantic --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics today: \n",
    "\n",
    "**1. Get the _kind_ of output you want**  \n",
    "**2. Minimize Hallucinations with response validators**\n",
    "\n",
    "By the end of the demo, you'll see some examples of how we can minimize hallucinations and gain more confidence in your model's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Guarantee\" Response Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConferencesYouShouldGoTo\nconferences\n  Field required [type=missing, input_value={'name': 'CypherCon', 'location': 'Wisconsin'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 26\u001b[0m\n\u001b[0;32m     14\u001b[0m     conferences: List[Conference]\n\u001b[0;32m     16\u001b[0m resp \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     ]\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mConferencesYouShouldGoTo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bdey\\OneDrive - Concurrency, Inc\\Projects\\cyphercon\\venv\\Lib\\site-packages\\pydantic\\main.py:538\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[1;34m(cls, json_data, strict, context)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    537\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ConferencesYouShouldGoTo\nconferences\n  Field required [type=missing, input_value={'name': 'CypherCon', 'location': 'Wisconsin'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "instruction = \"Return the `name`, and `location` of CypherCon, in a json object.\"\n",
    "\n",
    "class Conference(BaseModel):\n",
    "    name: str\n",
    "    location: str\n",
    "\n",
    "class ConferencesYouShouldGoTo(BaseModel):\n",
    "    conferences: List[Conference]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "ConferencesYouShouldGoTo.model_validate_json(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is fine, we might receive an output similar to \n",
    "\n",
    "```json\n",
    " {\n",
    "      \"name\": \"CypherCon\",\n",
    "      \"location\": \"Wisconsin, USA\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But the LLM may wrap it in markdown code blocks\n",
    "\n",
    "```python\n",
    "\n",
    "```json\n",
    " {\n",
    "      \"name\": \"CypherCon\",\n",
    "      \"location\": \"Wisconsin, USA\"\n",
    "}\n",
    "'''\n",
    "\n",
    ">>> JSONDecodeError: Expecting value: line 1 column 1 (char 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or it may respond with prose\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Ok here are the conferences and their locations:\n",
    "\n",
    " {\n",
    "      \"name\": \"CypherCon\",\n",
    "      \"location\": \"Wisconsin, USA\"\n",
    "}\n",
    "\"\"\")\n",
    ">>> JSONDecodeError: Expecting value: line 1 column 1 (char 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ^ content may contain valid JSON, but it isn't considered valid JSON without understanding the language model's behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining the api payload as a Pydantic model, we can leverage the `response_model` argument to instruct the model to generate the desired output. This is a powerful feature that allows us to generate structured data from any language model!\n",
    "\n",
    "Now, notice in this example that the prompts we use contain purely the data we want, where the tools and `tool_choice` now capture the schemas we want to output. This separation of concerns makes it much easier to organize the 'data' and the 'description' of the data that we want back out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConferencesYouShouldGoTo(conferences=[Conference(name='CypherCon', location='Wisconsin, USA'), Conference(name='ComicCon', location='San Diego, USA')])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"CypherCon and ComicCon\",\n",
    "        },\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"Requirements\",\n",
    "                \"description\": \"A list of conferences and their locations.\",\n",
    "                \"parameters\": ConferencesYouShouldGoTo.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\"name\": \"Requirements\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "ConferencesYouShouldGoTo.model_validate_json(\n",
    "    resp.choices[0].message.tool_calls[0].function.arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json {\n",
    "  \"conferences\": [\n",
    "    {\n",
    "      \"name\": \"CypherCon\",\n",
    "      \"location\": \"Wisconsin, USA\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"ComicCon\",\n",
    "      \"location\": \"San Diego, California, USA\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example we provided above is somewhat contrived, but it illustrates how Pydantic can be utilized to generate structured data from language models. Now, let's employ Instructor to streamline this process. Instructor is a compact library that enhances the OpenAI client by offering convenient features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Search query segmentation\n",
    "\n",
    "Let's consider a practical example. Imagine we have a search engine capable of comprehending intricate queries. For instance, if we make a request to find \"recent advancements in AI\", we could provide the following payload:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n",
    "  \"published_daterange\": {\n",
    "    \"start\": \"2023-09-17\",\n",
    "    \"end\": \"2021-06-17\"\n",
    "  },\n",
    "  \"domains_allow_list\": [\"arxiv.org\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. We can model this structured output in Pydantic using the instructor library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    start: datetime.date\n",
    "    end: datetime.date\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    rewritten_query: str\n",
    "    published_daterange: DateRange\n",
    "    domains_allow_list: List[str]\n",
    "\n",
    "    async def execute():\n",
    "        # Return the search results of the rewritten query\n",
    "        return api.search(json=self.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern empowers us to restructure the user's query for improved performance, without requiring the user to understand the inner workings of the search backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchQuery(rewritten_query='latest developments in Artificial Intelligence and Machine Learning', published_daterange=DateRange(start=datetime.date(2023, 4, 4), end=datetime.date(2024, 4, 4)), domains_allow_list=[])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enables response_model in the openai client\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def search(query: str) -> SearchQuery:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        response_model=SearchQuery,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for a search engine. Today's date is {datetime.date.today()}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "search(\"recent advancements in AI and ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Demo 1\n",
    "\n",
    "By defining the api payload as a Pydantic model, we can leverage the `response_model` argument to instruct the model to generate the desired output. This is a powerful feature that allows us to generate structured data from any language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Minimize LLM Hallucinations\n",
    "\n",
    "**Goal: You know how to minimize LLM hallucinations using validators**\n",
    "\n",
    "## Intro to Validators\n",
    "Validators are functions that take a value, check a property, raise an error, and return a value. They can be used to enforce constraints on model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_function(value):\n",
    "    if condition(value):\n",
    "        raise ValueError(\"Value is not valid\")\n",
    "    return mutation(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, consider validating a name field. Hereâ€™s how you can enforce a space in the name using Annotated and AfterValidator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Brandon', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     age: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m     11\u001b[0m     name: Annotated[\u001b[38;5;28mstr\u001b[39m, AfterValidator(name_must_contain_space)] \n\u001b[1;32m---> 13\u001b[0m person \u001b[38;5;241m=\u001b[39m \u001b[43mUserDetail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBrandon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bdey\\OneDrive - Concurrency, Inc\\Projects\\cyphercon\\venv\\Lib\\site-packages\\pydantic\\main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Brandon', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated\n",
    "from pydantic import BaseModel, ValidationError, AfterValidator, ValidationInfo\n",
    "\n",
    "def name_must_contain_space(v: str) -> str:\n",
    "    if \" \" not in v:\n",
    "        raise ValueError(\"Name must contain a space.\")\n",
    "    return v.lower()\n",
    "\n",
    "class UserDetail(BaseModel):\n",
    "    age: int\n",
    "    name: Annotated[str, AfterValidator(name_must_contain_space)] \n",
    "\n",
    "person = UserDetail.model_validate({\"age\": 32, \"name\": \"Brandon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Driven Validators\n",
    "\n",
    "Validators can also be used to enforce context-specific constraints. \n",
    "\n",
    "Validator context becomes crucial in directing language models. It helps in excluding specific content (like competitor names) or focusing on relevant topics, which is particularly effective in question-answering scenarios.\n",
    "\n",
    "For instance, consider a validator that checks if a name is in a list of names, and raises an error if it isn't. Enhancing validators with `ValidationInfo` adds nuanced control. For example, removing dynamic stopwords from a text requires us to pass in some context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(v: str, info: ValidationInfo):\n",
    "    context = info.context\n",
    "    if context:\n",
    "        stopwords = context.get('stopwords', set())\n",
    "        v = ' '.join(w for w in v.split() if w.lower() not in stopwords)\n",
    "    return v\n",
    "\n",
    "class Response(BaseModel):\n",
    "    message: Annotated[str, AfterValidator(remove_stopwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing dynamic context to the validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='This is an example response'\n",
      "message='example response'\n"
     ]
    }
   ],
   "source": [
    "data = {'message': 'This is an example response'}\n",
    "\n",
    "print(Response.model_validate(data))  \n",
    "#> text='This is an example response'\n",
    "\n",
    "print(Response.model_validate(\n",
    "    data, context={\n",
    "        'stopwords': ['this', 'is', 'an'] \n",
    "    }))\n",
    "#> text='example response'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Using an LLM \n",
    "\n",
    "Some rules are easier to express using natural language. For instance, consider the following rule: **'don't say objectionable things'**. This rule is difficult to express using a validator function, but easy to express using natural language. We can use an LLM to generate a validator function from this rule.\n",
    "\n",
    "Consider this example where we want some light moderation on a question answering model. We want to ensure that the answer does not contain objectionable content. We can use an LLM to generate a validator function that checks if the answer contains objectionable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The phrase 'Sex, drugs, and rock'n roll' may be considered objectionable due to the mention of drugs and sex. [type=assertion_error, input_value=\"Sex, drugs, and rock'n roll\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     18\u001b[0m     answer: NoEvil\n\u001b[1;32m---> 20\u001b[0m \u001b[43mQuestionAnswer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the meaning of life?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex, drugs, and rock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn roll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bdey\\OneDrive - Concurrency, Inc\\Projects\\cyphercon\\venv\\Lib\\site-packages\\pydantic\\main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The phrase 'Sex, drugs, and rock'n roll' may be considered objectionable due to the mention of drugs and sex. [type=assertion_error, input_value=\"Sex, drugs, and rock'n roll\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "\n",
    "from openai import OpenAI\n",
    "from instructor import llm_validator\n",
    "from pydantic import BaseModel, BeforeValidator\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "NoEvil = Annotated[\n",
    "    str,\n",
    "    BeforeValidator(\n",
    "        llm_validator(\"don't say objectionable things\", client)\n",
    "    )]\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: NoEvil\n",
    "\n",
    "QuestionAnswer.model_validate({\n",
    "    \"question\": \"What is the meaning of life?\",\n",
    "    \"answer\": \"Sex, drugs, and rock'n roll\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ValidationError: 1 validation error for QuestionAnswer\n",
    "answer\n",
    "  Assertion failed, The phrase 'Sex, drugs, and rock'n roll' may be considered objectionable due to the mention of drugs and sex. [type=assertion_error, input_value=\"Sex, drugs, and rock'n roll\", input_type=str]\n",
    "    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding responses in context\n",
    "\n",
    "Many organizations worry about hallucinations in their llm responses. To address this we can use validators to ensure that the model's responses are grounded in the context used to generate the prompt.\n",
    "\n",
    "For instance, let's consider a question-answering model that provides answers based on a text chunk. To ensure that the model's response is firmly based on the given text chunk, we can employ a validator. In this case, we can use `ValidationInfo` to verify the response. By using a straightforward validator, we can guarantee that the model's response is firmly grounded in the provided text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_exists(v: str, info: ValidationInfo):\n",
    "    context = info.context\n",
    "    if context:\n",
    "        context = context.get(\"text_chunk\")\n",
    "        if v not in context: # (1)!\n",
    "            raise ValueError(f\"Citation `{v}` not found in text\")\n",
    "    return v\n",
    "\n",
    "Citation = Annotated[str, AfterValidator(citation_exists)]\n",
    "\n",
    "class AnswerWithCitation(BaseModel):\n",
    "    answer: str\n",
    "    citation: Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets consider an example where we want to answer a question using a text chunk. We can use a validator to ensure that the model's response is grounded in the provided text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Madison is the capital.` not found in text [type=value_error, input_value='Madison is the capital.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m text_chunk\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease note that currently, Madison no longer is the capital of Wisconsin; Milwaukee is.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the capital of wisconsin?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mAnswerWithCitation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe capital of Wisconsin is Madison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcitation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMadison is the capital.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_chunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bdey\\OneDrive - Concurrency, Inc\\Projects\\cyphercon\\venv\\Lib\\site-packages\\pydantic\\main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Madison is the capital.` not found in text [type=value_error, input_value='Madison is the capital.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error"
     ]
    }
   ],
   "source": [
    "text_chunk= \"please note that currently, Madison no longer is the capital of Wisconsin; Milwaukee is.\"\n",
    "\n",
    "q = \"what is the capital of wisconsin?\"\n",
    "\n",
    "AnswerWithCitation.model_validate({\n",
    "    \"answer\": \"The capital of Wisconsin is Madison\",\n",
    "    \"citation\": \"Madison is the capital.\"\n",
    "}, context={\"text_chunk\": text_chunk})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alhought the answer in this example was correct, the validator will raise an error because the citation is not in the text chunk. Which can help us identify and correct the model's 'hallucination' which can not be defined as incorrectly cited information.\n",
    "\n",
    "We can use OpenAI to generate a response to a question using a text chunk. We can use a validator to ensure that the model's response is grounded in the provided text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what is the capital of wisconsin?, \n",
      "      text_chunk: please note that currently, Madison no longer is the capital of Wisconsin; Milwaukee is.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"question: {q}, \n",
    "      text_chunk: {text_chunk}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithCitation(answer='Milwaukee', citation='please note that currently, Madison no longer is the capital of Wisconsin; Milwaukee is.')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=AnswerWithCitation,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\n",
    "    ],\n",
    "    validation_context={\"text_chunk\": text_chunk},\n",
    ")\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion to Demo 2\n",
    "The power of these techniques lies in the flexibility and precision with which we can use Pydantic to describe and control outputs.\n",
    "\n",
    "Whether it's moderating content, avoiding specific topics or competitors, or even ensuring responses are grounded in provided context, Pydantic's `BaseModel` offers a very natural way to describe the data structure we want, while validation functions and ValidationInfo provide the flexibility to enforce these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* [Steering LLMs wih Pydantic](https://blog.pydantic.dev/blog/2024/01/04/steering-large-language-models-with-pydantic/)\n",
    "* [Pydantic docs](https://docs.pydantic.dev/latest/concepts/validators/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
